import requests
from bs4 import BeautifulSoup
import json
import urllib.parse
from typing import List, Dict, Optional
import re
import base64
import os

def search_duckduckgo(query: str, max_results: int = 3) -> str:
    """
    ä½¿ç”¨ DuckDuckGo æœç´¢ï¼ˆæ— éœ€ API å¯†é’¥ï¼‰
    :param query: æœç´¢æŸ¥è¯¢
    :param max_results: æœ€å¤§ç»“æœæ•°
    :return: æœç´¢ç»“æœçš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
    """
    try:
        # DuckDuckGo HTML æœç´¢
        encoded_query = urllib.parse.quote(query)
        url = f"https://html.duckduckgo.com/html/?q={encoded_query}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        results = []
        # æŸ¥æ‰¾æœç´¢ç»“æœ
        result_links = soup.find_all('a', class_='result__a')[:max_results]
        
        for i, link in enumerate(result_links, 1):
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            # è·å–æè¿°æ–‡æœ¬
            parent = link.find_parent('div', class_='results_links')
            snippet = ""
            if parent:
                snippet_elem = parent.find('a', class_='result__snippet')
                if snippet_elem:
                    snippet = snippet_elem.get_text(strip=True)
            
            results.append(f"{i}. **{title}**\n   {snippet}\n   é“¾æ¥: {href}")
        
        if results:
            return f"DuckDuckGo æœç´¢ç»“æœ '{query}':\n\n" + "\n\n".join(results)
        else:
            return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœã€‚"
            
    except Exception as e:
        return f"DuckDuckGo æœç´¢å‡ºé”™: {str(e)}"

def search_wikipedia(query: str, language: str = "zh") -> str:
    """
    æœç´¢ç»´åŸºç™¾ç§‘
    :param query: æœç´¢æŸ¥è¯¢
    :param language: è¯­è¨€ä»£ç ï¼ˆzh=ä¸­æ–‡, en=è‹±æ–‡ï¼‰
    :return: ç»´åŸºç™¾ç§‘æ‘˜è¦
    """
    try:
        # ç»´åŸºç™¾ç§‘ API
        api_url = f"https://{language}.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(query)}"
        headers = {"User-Agent": "MultiAgentBot/1.0"}
        
        response = requests.get(api_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            title = data.get('title', query)
            extract = data.get('extract', 'æœªæ‰¾åˆ°æ‘˜è¦')
            
            result = f"**ç»´åŸºç™¾ç§‘: {title}**\n\n{extract}"
            
            # å¦‚æœæœ‰ç›¸å…³é“¾æ¥
            if 'content_urls' in data:
                desktop_url = data['content_urls']['desktop']['page']
                result += f"\n\næ›´å¤šä¿¡æ¯: {desktop_url}"
                
            return result
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æœç´¢
            search_url = f"https://{language}.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'search',
                'srsearch': query,
                'format': 'json',
                'srlimit': 1
            }
            
            search_response = requests.get(search_url, params=params, headers=headers, timeout=10)
            if search_response.status_code == 200:
                search_data = search_response.json()
                if search_data['query']['search']:
                    first_result = search_data['query']['search'][0]
                    return search_wikipedia(first_result['title'], language)
            
            return f"æœªåœ¨ç»´åŸºç™¾ç§‘æ‰¾åˆ°å…³äº '{query}' çš„æ¡ç›®ã€‚"
            
    except Exception as e:
        return f"ç»´åŸºç™¾ç§‘æœç´¢å‡ºé”™: {str(e)}"

def extract_webpage_content(url: str, max_length: int = 1000) -> str:
    """
    æå–ç½‘é¡µçš„ä¸»è¦å†…å®¹
    :param url: ç½‘é¡µ URL
    :param max_length: æœ€å¤§å†…å®¹é•¿åº¦
    :return: æå–çš„å†…å®¹
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style"]):
            script.decompose()
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        content_areas = soup.find_all(['article', 'main', 'div'], class_=re.compile('content|article|main|body'))
        
        if content_areas:
            text = ' '.join([area.get_text(separator=' ', strip=True) for area in content_areas])
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šåŒºåŸŸï¼Œè·å–æ‰€æœ‰æ®µè½
            paragraphs = soup.find_all('p')
            text = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)
        text = text[:max_length] + "..." if len(text) > max_length else text
        
        return f"ç½‘é¡µå†…å®¹æ‘˜è¦ ({url}):\n\n{text}"
        
    except Exception as e:
        return f"æ— æ³•æå–ç½‘é¡µå†…å®¹: {str(e)}"

def search_web(query: str) -> str:
    """
    ç»¼åˆç½‘ç»œæœç´¢åŠŸèƒ½ï¼Œå°è¯•å¤šä¸ªæœç´¢æº
    :param query: The search query.
    :return: The content of the top search result.
    """
    results = []
    
    # 1. å°è¯• DuckDuckGo æœç´¢
    ddg_result = search_duckduckgo(query, max_results=2)
    if "æœç´¢ç»“æœ" in ddg_result:
        results.append(ddg_result)
    
    # 2. å°è¯•ç»´åŸºç™¾ç§‘ï¼ˆå¦‚æœæŸ¥è¯¢çœ‹èµ·æ¥åƒæ˜¯å¯»æ‰¾å®šä¹‰æˆ–è§£é‡Šï¼‰
    if any(keyword in query.lower() for keyword in ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'ä»‹ç»', 'what is', 'define']):
        wiki_result = search_wikipedia(query)
        if "ç»´åŸºç™¾ç§‘" in wiki_result:
            results.append(wiki_result)
    
    # 3. å¦‚æœæœ‰ç»“æœï¼Œè¿”å›ç»„åˆç»“æœ
    if results:
        return "\n\n---\n\n".join(results)
    else:
        return f"æœªèƒ½æ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ›´æ”¹æœç´¢è¯æˆ–ä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢ã€‚"

def search_news(query: str) -> str:
    """
    æœç´¢æœ€æ–°æ–°é—»ï¼ˆä½¿ç”¨ Google News RSSï¼‰
    :param query: æ–°é—»æŸ¥è¯¢
    :return: æ–°é—»ç»“æœ
    """
    try:
        # Google News RSS (æ— éœ€ API)
        encoded_query = urllib.parse.quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        
        items = soup.find_all('item')[:5]  # è·å–å‰5æ¡æ–°é—»
        
        if not items:
            return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æ–°é—»ã€‚"
        
        news_results = [f"æœ€æ–°æ–°é—»å…³äº '{query}':\n"]
        
        for i, item in enumerate(items, 1):
            title = item.find('title').text if item.find('title') else "æ— æ ‡é¢˜"
            pub_date = item.find('pubDate').text if item.find('pubDate') else "æ—¥æœŸæœªçŸ¥"
            link = item.find('link').text if item.find('link') else "#"
            
            # ç®€åŒ–æ—¥æœŸæ ¼å¼
            try:
                from datetime import datetime
                date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                pub_date = date_obj.strftime('%Y-%m-%d %H:%M')
            except:
                pass
            
            news_results.append(f"{i}. **{title}**\n   å‘å¸ƒæ—¶é—´: {pub_date}\n   é“¾æ¥: {link}")
        
        return "\n\n".join(news_results)
        
    except Exception as e:
        return f"æ–°é—»æœç´¢å‡ºé”™: {str(e)}"

def get_exchange_rate(base_currency: str, target_currency: str) -> str:
    """
    Get the exchange rate between two currencies.
    :param base_currency: The base currency (e.g., "USD").
    :param target_currency: The target currency (e.g., "CNY").
    :return: The exchange rate as a string.
    """
    url = f"https://open.er-api.com/v6/latest/{base_currency}"
    try:
        response = requests.get(url)
        data = response.json()
        if response.status_code == 200 and 'rates' in data:
            exchange_rate = data['rates'].get(target_currency)
            if exchange_rate:
                return f"The exchange rate from {base_currency} to {target_currency} is: {exchange_rate}"
            else:
                return f"Could not retrieve the exchange rate for {target_currency}."
        else:
            return f"Error fetching exchange rates: {data.get('error-type', 'Unknown error')}"
    except Exception as e:
        return f"An error occurred: {e}"

def get_weather(location: str, lang: str = "zh") -> str:
    """
    è·å–æŒ‡å®šåŸå¸‚çš„å®æ—¶å¤©æ°”ä¿¡æ¯
    :param location: åŸå¸‚åç§°ï¼ˆæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ï¼‰
    :param lang: è¯­è¨€ä»£ç ï¼Œé»˜è®¤ 'zh' ä¸­æ–‡
    :return: æ ¼å¼åŒ–çš„å¤©æ°”ä¿¡æ¯
    """
    try:
        # æ–¹æ¡ˆ1: ä½¿ç”¨ wttr.in APIï¼ˆæ— éœ€å¯†é’¥ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        # wttr.in æ”¯æŒå¤šè¯­è¨€å’Œ JSON æ ¼å¼
        url = f"https://wttr.in/{urllib.parse.quote(location)}?format=j1&lang={lang}"
        
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            # æå–å½“å‰å¤©æ°”ä¿¡æ¯
            current = data.get("current_condition", [{}])[0]
            weather_desc = current.get("lang_zh", [{}])[0].get("value", "") if lang == "zh" else current.get("weatherDesc", [{}])[0].get("value", "")
            if not weather_desc and lang == "zh":
                # å¦‚æœæ²¡æœ‰ä¸­æ–‡æè¿°ï¼Œä½¿ç”¨è‹±æ–‡å¹¶ç¿»è¯‘å¸¸è§å¤©æ°”
                weather_desc = current.get("weatherDesc", [{}])[0].get("value", "")
                weather_desc = translate_weather(weather_desc)
            
            temp_c = current.get("temp_C", "N/A")
            feels_like_c = current.get("FeelsLikeC", "N/A")
            humidity = current.get("humidity", "N/A")
            wind_speed = current.get("windspeedKmph", "N/A")
            wind_dir = current.get("winddir16Point", "N/A")
            pressure = current.get("pressure", "N/A")
            visibility = current.get("visibility", "N/A")
            
            # è·å–ä»Šæ—¥å¤©æ°”é¢„æŠ¥
            today_weather = data.get("weather", [{}])[0]
            max_temp = today_weather.get("maxtempC", "N/A")
            min_temp = today_weather.get("mintempC", "N/A")
            
            # è·å–é™æ°´æ¦‚ç‡
            hourly = today_weather.get("hourly", [])
            if hourly:
                rain_chances = [int(h.get("chanceofrain", 0)) for h in hourly]
                avg_rain_chance = sum(rain_chances) // len(rain_chances) if rain_chances else 0
            else:
                avg_rain_chance = 0
            
            # æ ¼å¼åŒ–è¾“å‡º
            result = f"""**{location}å®æ—¶å¤©æ°”**
å½“å‰æ—¶é—´ï¼š{data.get("current_condition", [{}])[0].get("localObsDateTime", "æœªçŸ¥")}

ğŸŒ¡ï¸ **æ¸©åº¦**ï¼š{temp_c}Â°Cï¼ˆä½“æ„Ÿ {feels_like_c}Â°Cï¼‰
ğŸŒ¤ï¸ **å¤©æ°”**ï¼š{weather_desc}
ğŸ’¨ **é£å†µ**ï¼š{translate_wind_dir(wind_dir)} {wind_speed} km/h
ğŸ’§ **æ¹¿åº¦**ï¼š{humidity}%
ğŸŒ§ï¸ **é™æ°´æ¦‚ç‡**ï¼š{avg_rain_chance}%
ğŸ“Š **æ°”å‹**ï¼š{pressure} hPa
ğŸ‘ï¸ **èƒ½è§åº¦**ï¼š{visibility} km

ğŸ“… **ä»Šæ—¥é¢„æŠ¥**
æœ€é«˜æ¸©åº¦ï¼š{max_temp}Â°C
æœ€ä½æ¸©åº¦ï¼š{min_temp}Â°C

æ•°æ®æ¥æºï¼šwttr.in"""
            
            return result
            
        else:
            # å¦‚æœ wttr.in å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            return search_weather_fallback(location)
            
    except Exception as e:
        # å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
        return search_weather_fallback(location)

def translate_weather(weather_en: str) -> str:
    """ç¿»è¯‘å¸¸è§å¤©æ°”æè¿°"""
    translations = {
        "Clear": "æ™´",
        "Sunny": "æ™´",
        "Partly cloudy": "å¤šäº‘",
        "Cloudy": "é˜´",
        "Overcast": "é˜´å¤©",
        "Mist": "è–„é›¾",
        "Patchy rain possible": "å±€éƒ¨å¯èƒ½æœ‰é›¨",
        "Patchy snow possible": "å±€éƒ¨å¯èƒ½æœ‰é›ª",
        "Patchy sleet possible": "å±€éƒ¨å¯èƒ½æœ‰é›¨å¤¹é›ª",
        "Thundery outbreaks possible": "å¯èƒ½æœ‰é›·æš´",
        "Blowing snow": "é£é›ª",
        "Blizzard": "æš´é£é›ª",
        "Fog": "é›¾",
        "Freezing fog": "å†»é›¾",
        "Patchy light drizzle": "å±€éƒ¨å°é›¨",
        "Light drizzle": "å°é›¨",
        "Heavy rain": "å¤§é›¨",
        "Light rain": "å°é›¨",
        "Moderate rain": "ä¸­é›¨",
        "Heavy snow": "å¤§é›ª",
        "Light snow": "å°é›ª",
        "Moderate snow": "ä¸­é›ª"
    }
    return translations.get(weather_en, weather_en)

def translate_wind_dir(wind_dir: str) -> str:
    """ç¿»è¯‘é£å‘"""
    translations = {
        "N": "åŒ—é£",
        "NNE": "ä¸œåŒ—ååŒ—é£",
        "NE": "ä¸œåŒ—é£",
        "ENE": "ä¸œåŒ—åä¸œé£",
        "E": "ä¸œé£",
        "ESE": "ä¸œå—åä¸œé£",
        "SE": "ä¸œå—é£",
        "SSE": "ä¸œå—åå—é£",
        "S": "å—é£",
        "SSW": "è¥¿å—åå—é£",
        "SW": "è¥¿å—é£",
        "WSW": "è¥¿å—åè¥¿é£",
        "W": "è¥¿é£",
        "WNW": "è¥¿åŒ—åè¥¿é£",
        "NW": "è¥¿åŒ—é£",
        "NNW": "è¥¿åŒ—ååŒ—é£"
    }
    return translations.get(wind_dir, wind_dir)

def search_weather_fallback(location: str) -> str:
    """å¤‡ç”¨å¤©æ°”æœç´¢æ–¹æ¡ˆï¼šé€šè¿‡ DuckDuckGo æœç´¢å¤©æ°”ä¿¡æ¯"""
    try:
        # ä½¿ç”¨ DuckDuckGo æœç´¢å¤©æ°”
        search_query = f"{location} å¤©æ°” å®æ—¶"
        encoded_query = urllib.parse.quote(search_query)
        
        # DuckDuckGo æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å¤©æ°”æŸ¥è¯¢ç«¯ç‚¹
        url = f"https://duckduckgo.com/?q={encoded_query}&ia=weather"
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å°è¯•æå–å¤©æ°”ä¿¡æ¯
        weather_module = soup.find('div', class_='module--weather')
        if weather_module:
            # æå–æ¸©åº¦
            temp_elem = weather_module.find('span', class_='module__temperature')
            temp = temp_elem.text.strip() if temp_elem else "æœªçŸ¥"
            
            # æå–å¤©æ°”æè¿°
            desc_elem = weather_module.find('div', class_='module__weather-type')
            weather_desc = desc_elem.text.strip() if desc_elem else "æœªçŸ¥"
            
            # æå–å…¶ä»–ä¿¡æ¯
            details = weather_module.find_all('span', class_='module__weather-detail')
            detail_text = " | ".join([d.text.strip() for d in details]) if details else ""
            
            return f"""**{location}å¤©æ°”ä¿¡æ¯**ï¼ˆæ¥æºï¼šDuckDuckGoï¼‰

ğŸŒ¡ï¸ æ¸©åº¦ï¼š{temp}
ğŸŒ¤ï¸ å¤©æ°”ï¼š{weather_desc}
ğŸ“Š å…¶ä»–ä¿¡æ¯ï¼š{detail_text}

æç¤ºï¼šç”±äºä½¿ç”¨äº†æœç´¢å¼•æ“æ•°æ®ï¼Œä¿¡æ¯å¯èƒ½ä¸å¤Ÿå®Œæ•´ã€‚å»ºè®®è®¿é—®ä¸“ä¸šå¤©æ°”ç½‘ç«™è·å–æ›´è¯¦ç»†ä¿¡æ¯ã€‚"""
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¤©æ°”æ¨¡å—ï¼Œå°è¯•é€šç”¨æœç´¢
        return f"""æœªèƒ½è·å– {location} çš„å®æ—¶å¤©æ°”æ•°æ®ã€‚

å»ºè®®æ‚¨é€šè¿‡ä»¥ä¸‹æ–¹å¼æŸ¥è¯¢ï¼š
1. è®¿é—®ä¸­å›½å¤©æ°”ç½‘ï¼šhttp://www.weather.com.cn
2. ä½¿ç”¨æ‰‹æœºè‡ªå¸¦å¤©æ°”åº”ç”¨
3. æœç´¢"{location}å¤©æ°”"è·å–æœ€æ–°ä¿¡æ¯

æŠ€æœ¯æç¤ºï¼šå¤©æ°”APIæš‚æ—¶æ— æ³•è®¿é—®ï¼Œè¯·ç¨åå†è¯•ã€‚"""
        
    except Exception as e:
        return f"è·å–å¤©æ°”ä¿¡æ¯æ—¶å‡ºé”™ï¼š{str(e)}\n\nè¯·å°è¯•ç›´æ¥è®¿é—®å¤©æ°”ç½‘ç«™æŸ¥è¯¢ã€‚"

def open_web_page(url: str, action: str = "screenshot") -> str:
    """
    æ‰“å¼€ç½‘é¡µå¹¶æ‰§è¡Œæ“ä½œï¼ˆæˆªå›¾ã€è·å–å†…å®¹ç­‰ï¼‰
    é›†æˆ MCP browser-tools æœåŠ¡å™¨åŠŸèƒ½
    :param url: è¦æ‰“å¼€çš„ç½‘é¡µURL
    :param action: æ“ä½œç±»å‹ ("screenshot", "content", "logs")
    :return: æ“ä½œç»“æœ
    """
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ MCP browser-tools æœåŠ¡å™¨
        try:
            # å¯¼å…¥ MCP å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            from use_mcp_tool import use_mcp_tool
            
            # ä½¿ç”¨ MCP browser-tools æœåŠ¡å™¨æˆªå›¾
            if action == "screenshot":
                result = use_mcp_tool(
                    "github.com/AgentDeskAI/browser-tools-mcp",
                    "takeScreenshot",
                    {}
                )
                
                if "æˆåŠŸ" in str(result) or "success" in str(result).lower():
                    return f"âœ… ç½‘é¡µæˆªå›¾æˆåŠŸ\nç½‘å€ï¼š{url}\nç»“æœï¼š{result}"
                else:
                    # å¦‚æœæˆªå›¾å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ
                    return open_web_page_fallback(url, action)
            
            elif action == "logs":
                console_logs = use_mcp_tool(
                    "github.com/AgentDeskAI/browser-tools-mcp",
                    "getConsoleLogs",
                    {}
                )
                return f"ğŸ“‹ æµè§ˆå™¨æ§åˆ¶å°æ—¥å¿—\nç½‘å€ï¼š{url}\næ—¥å¿—ï¼š{console_logs}"
            
            elif action == "content":
                # å…ˆæˆªå›¾ï¼Œç„¶åè·å–å†…å®¹
                screenshot_result = use_mcp_tool(
                    "github.com/AgentDeskAI/browser-tools-mcp",
                    "takeScreenshot", 
                    {}
                )
                return f"ğŸ“„ ç½‘é¡µå†…å®¹è·å–\nç½‘å€ï¼š{url}\næˆªå›¾ï¼š{screenshot_result}\n\nå†…å®¹ï¼š{extract_webpage_content(url)}"
                
        except ImportError:
            # å¦‚æœ MCP ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            return open_web_page_fallback(url, action)
        except Exception as mcp_error:
            print(f"MCP browser-tools è°ƒç”¨å¤±è´¥: {mcp_error}")
            return open_web_page_fallback(url, action)
            
    except Exception as e:
        return f"âŒ æ‰“å¼€ç½‘é¡µå¤±è´¥ï¼š{str(e)}"

def open_web_page_fallback(url: str, action: str = "screenshot") -> str:
    """
    å¤‡ç”¨ç½‘é¡µè®¿é—®æ–¹æ¡ˆï¼šä½¿ç”¨ requests + BeautifulSoup
    :param url: ç½‘é¡µURL
    :param action: æ“ä½œç±»å‹
    :return: æ“ä½œç»“æœ
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        if action == "content":
            # æå–å¹¶è¿”å›é¡µé¢å†…å®¹
            content = extract_webpage_content(url, max_length=2000)
            return f"ğŸ“„ ç½‘é¡µå†…å®¹è·å–æˆåŠŸ\n{content}"
            
        elif action == "screenshot":
            # æ— æ³•çœŸæ­£æˆªå›¾ï¼Œä½†å¯ä»¥è¿”å›é¡µé¢åŸºæœ¬ä¿¡æ¯
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
            
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', 'æ— æè¿°') if meta_desc else "æ— æè¿°"
            
            # ç»Ÿè®¡é¡µé¢å…ƒç´ 
            links_count = len(soup.find_all('a'))
            images_count = len(soup.find_all('img'))
            
            return f"""ğŸ“± ç½‘é¡µè®¿é—®æˆåŠŸï¼ˆå¤‡ç”¨æ¨¡å¼ï¼‰
ç½‘å€ï¼š{url}
æ ‡é¢˜ï¼š{title_text}
æè¿°ï¼š{description}
çŠ¶æ€ï¼š{response.status_code} - é¡µé¢æ­£å¸¸åŠ è½½
å…ƒç´ ç»Ÿè®¡ï¼š{links_count} ä¸ªé“¾æ¥ï¼Œ{images_count} å¼ å›¾ç‰‡

âš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨å¤‡ç”¨æ¨¡å¼ï¼Œæ— æ³•æä¾›çœŸå®æˆªå›¾ã€‚
å¦‚éœ€å®Œæ•´æµè§ˆå™¨åŠŸèƒ½ï¼Œè¯·ç¡®ä¿ MCP browser-tools æœåŠ¡å™¨æ­£å¸¸è¿è¡Œã€‚"""
            
        elif action == "logs":
            return f"""ğŸ“‹ ç½‘é¡µè®¿é—®æ—¥å¿—ï¼ˆå¤‡ç”¨æ¨¡å¼ï¼‰
ç½‘å€ï¼š{url}
HTTPçŠ¶æ€ï¼š{response.status_code}
å“åº”å¤´ï¼š{dict(list(response.headers.items())[:5])}
é¡µé¢å¤§å°ï¼š{len(response.text)} å­—ç¬¦

âš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨å¤‡ç”¨æ¨¡å¼ï¼Œæ— æ³•è·å–æµè§ˆå™¨æ§åˆ¶å°æ—¥å¿—ã€‚"""
            
    except requests.RequestException as e:
        return f"âŒ ç½‘é¡µè®¿é—®å¤±è´¥ï¼š{str(e)}"
    except Exception as e:
        return f"âŒ å¤„ç†ç½‘é¡µæ—¶å‡ºé”™ï¼š{str(e)}"
